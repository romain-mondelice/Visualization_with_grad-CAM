{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights='ResNet34_Weights.IMAGENET1K_V1')  # New PyTorch interface for loading weights!\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224). \n",
    "\n",
    "First convolution layer with maxpool. \n",
    "Then 4 ResNet blocks. \n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7). \n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
    "\n",
    "##classes is a dictionary with the name of each class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # Note: If the inverse normalisation is required, apply 1/x to the above object\n",
    "    \n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def inverse_normalize(tensor):\n",
    "    # Convert tensor to numpy array\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    tensor = tensor.numpy()\n",
    "    \n",
    "    # Inverse normalization\n",
    "    for i in range(3):  # Loop over each channel\n",
    "        tensor[i] = (tensor[i] * std[i]) + mean[i]\n",
    "    tensor = np.clip(tensor, 0, 1)\n",
    "    \n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.exists(\"data/TP2_images\"):\n",
    "    os.mkdir(\"data/TP2_images\")\n",
    "    !cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 5\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)\n",
    "\n",
    "# shape\n",
    "print(dataset[index][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))  # the output is a tensor with 1000 elements, each one is the score of a class.there are 1000 classes in the imagenet dataset\n",
    "values, indices = torch.topk(output, 3) # the top 3 classes ; return a tuple of (values, indices)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy()) \n",
    "# print(\"probability\", F.softmax(output, dim=1)[0][indices[0]]) # the probability of the top 3 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
    " + More on [autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) and [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)\n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | - \n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, conv_layer):\n",
    "        '''\n",
    "        Initialize the GradCAM class.\n",
    "        Input: model (nn.Module), conv_layer (nn.Module)\n",
    "\n",
    "        '''\n",
    "        self.model = model # resnet34 model\n",
    "        self.hook_feature_map = [] # list of feature maps\n",
    "        self.hook_gradient = [] # list of gradients\n",
    "        self.conv_layer = conv_layer \n",
    "        self.conv_layer.register_forward_hook(self.forward_hook) # capture feature mapas\n",
    "        self.conv_layer.register_backward_hook(self.backward_hook) # capture gradients\n",
    "\n",
    "        # put the model in evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward_hook(self, model, input, output):\n",
    "        '''\n",
    "        Forward hook to capture the feature maps of the last rectified convolutional layer.\n",
    "        Input: model (nn.Module), input_tensor (tensor), output_tensor (tensor)\n",
    "        '''\n",
    "        self.hook_feature_map = output\n",
    "    \n",
    "    def backward_hook(self, model, input, output):\n",
    "        \n",
    "        '''\n",
    "        Backward hook to capture the gradients of the last rectified convolutional layer.\n",
    "        Input: model (nn.Module), input_tensor (tensor), output_tensor (tensor)\n",
    "        '''\n",
    "        self.hook_gradient = output[0] # the gradients are stored in the first element of the output tensor\n",
    "\n",
    "\n",
    "    def forward_pass(self, image):\n",
    "        '''\n",
    "        Forward pass the image in the model and get the top 3 labels with their scores.\n",
    "        Input: image (tensor)\n",
    "        Output: output (tensor), values (tensor), indices (tensor)\n",
    "        '''\n",
    "        output = self.model(image).squeeze() # 1000 probas\n",
    "        values, indices = torch.topk(output, k=3) # 3 tuples (13,3, 286)\n",
    "        last_feature_map = self.hook_feature_map.detach().squeeze()\n",
    "        return output, values, indices, last_feature_map\n",
    "\n",
    "    \n",
    "    def classes_activation_map(self, image):\n",
    "        '''\n",
    "        Generate the activation map for the top 3 classes.\n",
    "        Input: image (tensor)\n",
    "        Output: heatmaps (dict), indices (tensor), values (tensor)\n",
    "        '''\n",
    "        # Forward pass : We make a predicion on the image to retrieve the top 3 classes, their scores and feature maps of the last rectified convolutional layer\n",
    "        output, values, indices, last_feature_map = self.forward_pass(image)  \n",
    "        # We initialize the heatmaps dictionary to store the activation maps for the top 3 classes\n",
    "        heatmaps = [None, None, None]\n",
    "       # We iterate through the top 3 classes to compute the activation map for each class\n",
    "        for i, label in enumerate(indices):    \n",
    "            # We set the gradients of the output layer to zero every time we backpropagate\n",
    "            self.model.zero_grad() \n",
    "            # We encode the label to backpropagate only on the class of interest\n",
    "            encoded_label = torch.zeros(output.shape[0])\n",
    "            encoded_label[label] = 1\n",
    "            output.backward(encoded_label, retain_graph=True)\n",
    "            # We use the hook_gradient to retrieve the gradients of the last rectified convolutional layer\n",
    "            gradients = self.hook_gradient.detach()\n",
    "\n",
    "            # We perform global average pooling on the gradients to get the importance of each feature map\n",
    "            feature_pooling = F.adaptive_avg_pool2d(gradients, (1, 1)).squeeze()\n",
    "            \n",
    "            # We compute the weighted feature map by multiplying the importance of each feature map by the feature map itself\n",
    "            Weighted_feature_map = torch.sum(\n",
    "                feature_pooling.view(-1, 1, 1) * last_feature_map, dim=0\n",
    "            )\n",
    "            # dim 0 for summing the elements along the first dimension. \n",
    "            # This means tat it's adding together the values of all feature maps at each (x, y)  pixel\n",
    "            # We apply the ReLU activation function to the weighted feature map to get the activation map\n",
    "            relu = torch.nn.ReLU()\n",
    "            relu_heatmap = relu(Weighted_feature_map) # 1(channel) x 7x7  # Relu is used to remove the negative values\n",
    "            # project the heatmap to the original image size to visualize it better\n",
    "            heatmaps[i] = nn.functional.interpolate(relu_heatmap.unsqueeze(0).unsqueeze(0),(224,224),mode='bilinear').squeeze() # unsqueee to make it 1x1x7x7 \n",
    " \n",
    "        return heatmaps, indices, values,relu_heatmap\n",
    "    \n",
    "    \n",
    "\n",
    "    def plot_activation_maps(self, dataset):\n",
    "        '''\n",
    "        Plot the original images with their activation maps and the top 3 classes.\n",
    "        Input: dataset (torch.utils.data.Dataset)\n",
    "        '''\n",
    "        num_image = len(dataset)\n",
    "        fig, ax = plt.subplots(num_image, 4, figsize=(40,120))\n",
    "        for i in range(num_image):\n",
    "            image = dataset[i][0].view(1, 3, 224, 224)\n",
    "            original_image = dataset[i][0]  # Original image without view()\n",
    "            original_image_denormalize = inverse_normalize(original_image).transpose(1, 2, 0)\n",
    "\n",
    "            heatmaps, indices, values, relu_heatmap = self.classes_activation_map(image)\n",
    "            for j in range(4):  # 4 columns\n",
    "                if j == 3:  \n",
    "                    ax[i, j].imshow(original_image_denormalize)\n",
    "                    ax[i, j].imshow(relu_heatmap, cmap='rainbow')\n",
    "                    ax[i, j].set_title(\"\")\n",
    "                else:\n",
    "                    ax[i, j].imshow(original_image_denormalize)\n",
    "                    ax[i, j].imshow(heatmaps[j], cmap='rainbow', alpha=0.45)\n",
    "                    title1 = \" \\n Top {} class : {} \\n\".format(j+1, classes[int(indices[j])])\n",
    "                    title2 = \"Score: {}\".format(values[j])\n",
    "                    ax[i, j].set_title(title1 + title2)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "grad_CAM = GradCAM(resnet34, resnet34.layer4[2].bn2).plot_activation_maps(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "### Complementary questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "##### Try GradCAM on others convolutional layers, describe and comment the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_CAM = GradCAM(resnet34, resnet34.layer2)\n",
    "grad_CAM.plot_activation_maps(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the layers used for GradCAM, we find different attention maps. The more superficial layers capture specific elements of the image, while the deeper layers, such as resnet34.layer4[2].conv2 (the last convolution layer), capture generalized features of the image. So as we progress to the deeper layers of the network, the extracted features move away from specific details to more global patterns that represent higher-level concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "##### What are the principal contributions of GradCAM (the answer is in the paper) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "mxZr9jPfJMMl"
   },
   "source": [
    "**1- Broad Applicability**: Works with various CNN architectures without model-specific modifications, enhancing its versatility across different tasks.\n",
    "\n",
    "**2- Class-discriminative Localization**: Highlights image regions important for model decisions, aiding in understanding and diagnosing model behavior.\n",
    "\n",
    "**3- Enhanced Interpretability**: Offers visual explanations that improve the interpretability of deep learning models, crucial for sensitive applications.\n",
    "\n",
    "**4- Combinable with Other Techniques**: Can be combined with techniques like Guided Backpropagation for even more detailed visual explanations.\n",
    "\n",
    "**5- Ease of Implementation and Use**: Accessible and straightforward to implement, facilitating wider adoption without extensive computational demands.\n",
    "\n",
    "**6- Building Trust and Debugging**: Helps in building trust in AI systems and assists in debugging by visually indicating model focus areas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "mxZr9jPfJMMl",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
